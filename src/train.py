import torch
from torch.utils.data import DataLoader
import time
import os
import pandas as pd
from sklearn.metrics import average_precision_score, precision_recall_curve
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve

def train_loop(model,
               train_loader: DataLoader,
               val_loader: DataLoader = None,
               device='cuda',
               optimizer=None,
               criterion=None,
               num_epochs=10,
               scheduler=None,
               log_interval=100,
               save_interval=1,
               save_path=None):
    """
    é€šç”¨ PyTorch è¨“ç·´è¿´åœˆï¼Œé©ç”¨ CNN+LSTM äºŒåˆ†é¡æ¨¡å‹
    """

    model = model.to(device)
    print(f"Model moved to {device}")
    if save_path is None:
        save_path = time.strftime('model/%m%d_%H%M/', time.localtime())
    os.makedirs(save_path, exist_ok=True)
    
    if os.path.exists(save_path+'/training_log.csv'):
        log = pd.read_csv(save_path+'/training_log.csv')
        cur_epoch = int(log['epoch'].max() + 1)
        min_val_loss = log['val_loss'].min()
        print(f"Resuming training from epoch {cur_epoch}")
    else:
        log = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'aucpr', 'accuracy', 'time'])
        cur_epoch = 1
        min_val_loss = float('inf')

    for epoch in range(cur_epoch, cur_epoch + num_epochs):
        start_time = time.time()
        model.train()
        total_loss = 0

        for batch_idx, (X, y) in enumerate(train_loader):
            #print(X.shape, y.shape)  # Debugging shape
            X = X.to(device, non_blocking=True)  # å»ºè­°åŠ  non_blocking=True
            y = y.to(device, non_blocking=True)
            optimizer.zero_grad()
            output = model(X)
            output = output.squeeze()  # [B] if needed

            loss = criterion(output, y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if batch_idx % log_interval == 0:
                print(f"[Epoch {epoch}/{cur_epoch + num_epochs}] Step {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")

        avg_train_loss = total_loss / len(train_loader)
        print(f"ğŸ” Epoch {epoch} finished. Avg Train Loss: {avg_train_loss:.4f}")
        end_time = time.time()

        if (epoch) % save_interval == 0:
            torch.save(model.state_dict(), f'{save_path}/epoch_{epoch}.pth')
            log.to_csv(f'{save_path}/training_log.csv', index=False)
            #print(f"Model saved at epoch {epoch}")

        # ===== é©—è­‰éšæ®µ =====
        if val_loader is not None:
            model.eval()
            val_loss = 0.0
            correct = 0
            total = 0
            y_true_val_list, y_score_val_list = [], []

            # ä½ å¯ä»¥è‡ªè¨‚å¤šå€‹ Kï¼›ä¾‹å¦‚æŠ“æœ€é«˜åˆ†çš„ 50/100/200 ç­†
            topk_list = [50, 100, 200]

            with torch.inference_mode():
                for X_val, y_val in val_loader:
                    X_val = X_val.to(device, non_blocking=True)
                    y_val = y_val.float().view(-1).to(device, non_blocking=True)

                    # è‹¥è³‡æ–™å« -1ï¼ˆin-sessionï¼‰ï¼Œå…ˆéæ¿¾æ‰
                    valid_mask = (y_val == 0) | (y_val == 1)
                    if valid_mask.sum() == 0:
                        continue
                    
                    X_val = X_val[valid_mask]
                    y_val = y_val[valid_mask]

                    logits = model(X_val).view(-1)
                    val_loss += criterion(logits, y_val).item()

                    probs = torch.sigmoid(logits)  # [B]

                    # æ”¶é›†åˆ° CPU åšæ•´é«”æŒ‡æ¨™
                    y_true_val_list.append(y_val.detach().cpu().numpy().astype(np.int32))
                    y_score_val_list.append(probs.detach().cpu().numpy())

                    # Accuracyï¼ˆåƒ…ä¾›åƒè€ƒï¼‰
                    pred = (probs > 0.5).int()
                    correct += (pred == y_val.int()).sum().item()
                    total += y_val.size(0)

            if total > 0:
                avg_val_loss = val_loss / len(val_loader)
                accuracy = correct / total

                y_true_val = np.concatenate(y_true_val_list).reshape(-1)
                y_score_val = np.concatenate(y_score_val_list).reshape(-1)

                # ---- ä¸»æŒ‡æ¨™ï¼šAUCPR èˆ‡ baseline ----
                pos_rate = float((y_true_val == 1).mean())  # baselineï¼ˆrandom classifierï¼‰
                if pos_rate == 0.0:
                    aucpr = float('nan')
                    ap_uplift = float('nan')
                else:
                    aucpr = average_precision_score(y_true_val, y_score_val)
                    ap_uplift = aucpr / pos_rate

                # ---- Top-K æŒ‡æ¨™ ----
                # å…ˆä¾åˆ†æ•¸æ’åºï¼ˆç”±é«˜åˆ°ä½ï¼‰
                order = np.argsort(-y_score_val)
                y_true_sorted = y_true_val[order]

                total_pos = int((y_true_val == 1).sum())
                topk_metrics = {}
                for k in topk_list:
                    k_eff = min(k, len(y_true_sorted))
                    if k_eff == 0:
                        p_at_k = float('nan')
                        r_at_k = float('nan')
                    else:
                        tp_at_k = int(y_true_sorted[:k_eff].sum())
                        p_at_k = tp_at_k / k_eff
                        r_at_k = (tp_at_k / total_pos) if total_pos > 0 else float('nan')
                    topk_metrics[f'prec@{k}'] = p_at_k
                    topk_metrics[f'rec@{k}']  = r_at_k

                # ---- å°å‡ºæ‘˜è¦ ----
                k_str = " | ".join([f"P@{k}:{topk_metrics[f'prec@{k}']:.3f} R@{k}:{topk_metrics[f'rec@{k}']:.3f}" for k in topk_list])
                print(
                    f"âœ… Validation Loss: {avg_val_loss:.4f} | Acc: {accuracy:.2%} | "
                    f"AUC-PR: {aucpr:.4f} | baseline: {pos_rate:.4f} | uplift: {ap_uplift:.2f}x | {k_str}"
                )

                # ---- å„²å­˜ bestï¼ˆä»ä»¥ val_loss ç‚ºæº–ï¼›ä½ ä¹Ÿå¯æ”¹æˆä»¥ aucpr ç‚ºæº–ï¼‰----
                if avg_val_loss < min_val_loss:
                    min_val_loss = avg_val_loss
                    torch.save(model.state_dict(), f'{save_path}/best_model.pth')
                    print(f"Best model saved at epoch {epoch} with loss {avg_val_loss:.4f}")

                # ---- å°‡æ–°æŒ‡æ¨™å¯«å…¥ log ----
                # å‹•æ…‹è£œä¸Šæ–°æ¬„ä½ï¼ˆè‹¥ç¬¬ä¸€æ¬¡å¯«å…¥ï¼‰
                required_cols = ['epoch', 'train_loss', 'val_loss', 'aucpr', 'accuracy', 'baseline_pos_rate', 'ap_uplift', 'time']
                for k in topk_list:
                    required_cols += [f'prec@{k}', f'rec@{k}']
                for col in required_cols:
                    if col not in log.columns:
                        log[col] = np.nan  # å…ˆè£œæ¬„ä½

                row = {
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'aucpr': aucpr,
                    'accuracy': accuracy,
                    'baseline_pos_rate': pos_rate,
                    'ap_uplift': ap_uplift,
                    'time': end_time - start_time
                }
                for k in topk_list:
                    row[f'prec@{k}'] = topk_metrics[f'prec@{k}']
                    row[f'rec@{k}']  = topk_metrics[f'rec@{k}']
                log.loc[len(log)] = row

            else:
                print("âš ï¸ No valid samples in validation after filtering labels.")
                # ä¹ŸæŠŠç©ºå€¼å¯«é€² log ä»¥ä¿æŒ epoch å°é½Š
                if 'baseline_pos_rate' not in log.columns:
                    log['baseline_pos_rate'] = np.nan
                    log['ap_uplift'] = np.nan
                log.loc[len(log)] = [epoch, avg_train_loss, np.nan, np.nan, np.nan, end_time - start_time]

        # scheduler step if used
        if scheduler is not None:
            scheduler.step()


    log.to_csv(f'{save_path}/training_log.csv', index=False)
    torch.save(model.state_dict(), f'{save_path}/epoch_{epoch}.pth')
    print("ğŸ Training completed.")
    
    return log

def test_loop(model,
            test_loader: DataLoader,
            best_threshold=0.5,
            device='cuda',
            criterion=None):
    """
    æ¸¬è©¦è¿´åœˆï¼Œç”¨æ–¼è©•ä¼°æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾
    """
    import numpy as np
    model = model.to(device)
    model.eval()
    
    total_loss = 0
    correct = 0
    total = 0
    trues = []
    outputs = []
    predictions = []

    with torch.no_grad():
        for X_test, y_test in tqdm(test_loader, desc="Testing"):
            X_test, y_test = X_test.to(device), y_test.to(device).float()
            output = model(X_test).squeeze()
            outputs.append(torch.sigmoid(output).cpu().numpy())

            loss = criterion(output, y_test)
            total_loss += loss.item()

            pred = torch.sigmoid(output) > best_threshold  # äºŒåˆ†é¡é æ¸¬
            trues.append(y_test.cpu().numpy())
            predictions.append(pred.cpu().numpy())
            correct += (pred == y_test).sum().item()
            total += y_test.size(0)

    avg_test_loss = total_loss / len(test_loader)
    accuracy = correct / total
    print(f"ğŸ” Test Loss: {avg_test_loss:.4f} | Accuracy: {accuracy:.2%}")
    return np.concatenate(trues), np.concatenate(predictions), np.concatenate(outputs)

def generate_report(trues, predictions, outputs):
    """
    è©•ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè¨ˆç®— AUC-PR å’Œå…¶ä»–æŒ‡æ¨™
    """
    print(classification_report(trues, predictions , target_names=['Normal', 'Failure']))
    print(confusion_matrix(trues, predictions ))
    roc_auc = roc_auc_score(trues, outputs)
    print(f"ROC AUC: {roc_auc:.4f}")
    curve = roc_curve(trues, outputs)

    plt.figure(figsize=(8, 6))
    plt.plot(curve[0], curve[1], label='ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')

def evaluate_model(model,
            test_loader: DataLoader,
            best_threshold=0.5,
            device='cuda',
            criterion=None):
    trues, predictions, outputs = test_loop(model, test_loader, best_threshold, device, criterion)
    generate_report(trues, predictions, outputs)