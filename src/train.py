import torch
from torch.utils.data import DataLoader

def train_loop(model,
               train_loader: DataLoader,
               val_loader: DataLoader = None,
               device='cuda',
               optimizer=None,
               criterion=None,
               num_epochs=10,
               time_step=30,
               scheduler=None,
               log_interval=100):
    """
    é€šç”¨ PyTorch è¨“ç·´è¿´åœˆï¼Œé©ç”¨ CNN+LSTM äºŒåˆ†é¡æ¨¡å‹
    """

    model = model.to(device)

    for epoch in range(1, num_epochs + 1):
        model.train()
        total_loss = 0

        for batch_idx, (X, y) in enumerate(train_loader):
            X, y = X.to(device), y.to(device).float()

            optimizer.zero_grad()
            output = model(X)
            output = output.squeeze()  # [B] if needed

            loss = criterion(output, y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if batch_idx % log_interval == 0:
                print(f"[Epoch {epoch}/{num_epochs}] Step {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}")

        avg_train_loss = total_loss / len(train_loader)
        print(f"ğŸ” Epoch {epoch} finished. Avg Train Loss: {avg_train_loss:.4f}")

        # é©—è­‰éšæ®µ
        if val_loader is not None:
            model.eval()
            val_loss = 0
            correct = 0
            total = 0

            with torch.no_grad():
                for X_val, y_val in val_loader:
                    X_val, y_val = X_val.to(device), y_val.to(device).float()
                    output = model(X_val).squeeze()
                    val_loss += criterion(output, y_val).item()

                    pred = torch.sigmoid(output) > 0.5
                    correct += (pred == y_val).sum().item()
                    total += y_val.size(0)

            avg_val_loss = val_loss / len(val_loader)
            accuracy = correct / total
            print(f"âœ… Validation Loss: {avg_val_loss:.4f} | Accuracy: {accuracy:.2%}")

        # scheduler step if used
        if scheduler is not None:
            scheduler.step()

    print("ğŸ Training completed.")

if __name__ == "__main__":
    from .model import CNNLSTMModel as Model
    from .data_loader import InverterTimeSeriesDataset


    # å‡è¨­ä½ æœ‰ä¸€å€‹è™•ç†éçš„ dataframeï¼š
    # - å·²æœ‰æ™‚é–“é †åº
    # - å·²è£œå€¼/æ¨™æº–åŒ–
    # - æœ‰ label æ¬„ä½
    # - æœ‰ feature æ¬„ä½ï¼ˆä½ å·²é¸å¥½ï¼‰

    feature_cols = ['AC_POWER']  # ä½ è¦ç”¨çš„æ¬„ä½ï¼Œä¾‹å¦‚ ['AC_POWER', 'DC_VOLTAGE', ...]
    num_features = len(feature_cols)
    label_col = 'label'

    dataset = InverterTimeSeriesDataset(
        dataframe=processed_df,
        feature_cols=feature_cols,
        label_col=label_col,
        window_size=30,
        stride=1
    )

    train_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)

    model = Model(
        num_features=num_features,
        cnn_out_channels=32,
        lstm_hidden_size=64,
        lstm_layers=1,
        dropout=0.3
    )
    
    train_loop(
        model=model,
        train_loader=train_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        criterion=torch.nn.BCELoss(),
        num_epochs=10,
        log_interval=10
    )